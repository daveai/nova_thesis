\chapter{Literature Review}

This chapter surveys the current state of knowledge regarding volatility forecasting. It begins with the fundamentals: why volatility matters and what makes it difficult to predict. From there, it traces the development of GARCH-family models, which have dominated the field for decades, before turning to machine learning alternatives that have gained traction more recently. The chapter concludes by identifying gaps in the literature that this thesis aims to address.

\section{Volatility in Financial Markets}

At its simplest, volatility is the standard deviation of returns. This apparent simplicity, however, can be misleading. Volatility sits at the heart of option pricing, risk management, and portfolio construction \parencite{hull2018}. Underestimating it may lead to excessive risk exposure; overestimating it can result in suboptimal capital allocation.

What makes volatility tricky to forecast is that it does not behave like a well-mannered statistical object. Returns cluster: calm periods follow calm periods, and turbulent stretches tend to persist \parencite{poon2003}. The distribution of returns is fat-tailed, meaning extreme moves happen more often than a normal distribution would suggest. And there is asymmetry: bad news tends to rattle markets more than good news of equivalent magnitude. These three stylised facts (clustering, fat tails, asymmetry) are well established. Any model worth using needs to accommodate them, at least to some degree.

\section{Econometric Approaches: The GARCH Family}

\subsection{ARCH and GARCH Models}

The modern treatment of volatility can be traced to \textcite{engle1982}, who proposed the Autoregressive Conditional Heteroskedasticity (ARCH) model. The core idea appears straightforward in retrospect: if volatility clusters, then current variance should depend on past shocks. Formally:

\begin{equation}
    \sigma_t^2 = \omega + \sum_{i=1}^{q} \alpha_i \epsilon_{t-i}^2
    \label{eq:arch}
\end{equation}

Here $\sigma_t^2$ is the conditional variance at time $t$, $\omega$ is a baseline level, and the $\alpha_i$ terms pick up the influence of past squared residuals.

\textcite{bollerslev1986} extended this by letting variance depend on its own lagged values, yielding the Generalised ARCH (or GARCH) model:

\begin{equation}
    \sigma_t^2 = \omega + \sum_{i=1}^{q} \alpha_i \epsilon_{t-i}^2 + \sum_{j=1}^{p} \beta_j \sigma_{t-j}^2
    \label{eq:garch}
\end{equation}

The GARCH(1,1) version, with just one lag of each type, has proven surprisingly durable. It remains the workhorse model in both academia and industry \parencite{engle2001}. The sum $\alpha_1 + \beta_1$ measures persistence: values near one mean that volatility shocks take a long time to die out.

\subsection{Extensions: EGARCH and Asymmetric Effects}

Standard GARCH treats positive and negative shocks symmetrically. In practice, that is often wrong. A 3\% drop tends to unsettle markets more than a 3\% gain, the so-called leverage effect. \textcite{nelson1991} addressed this with the Exponential GARCH (EGARCH) specification:

\begin{equation}
    \ln(\sigma_t^2) = \omega + \sum_{i=1}^{q} \left[ \alpha_i \left| \frac{\epsilon_{t-i}}{\sigma_{t-i}} \right| + \gamma_i \frac{\epsilon_{t-i}}{\sigma_{t-i}} \right] + \sum_{j=1}^{p} \beta_j \ln(\sigma_{t-j}^2)
    \label{eq:egarch}
\end{equation}

The $\gamma$ parameter captures asymmetry. When $\gamma$ is negative, negative returns tend to raise volatility more than positive returns of equivalent magnitude, consistent with patterns typically observed in equity markets.

\subsection{Strengths and Limitations}

GARCH models have endured for sound reasons. The parameters carry economic interpretation: shock persistence, news impact, and asymmetric responses can all be discussed in terms that make economic sense. Estimation is relatively efficient, and forecasts can be computed analytically \parencite{bollerslev1992}. For risk managers who must explain their models to regulators, this transparency is valuable.

That said, GARCH is not without problems. The models impose a specific functional form on volatility dynamics, which may miss subtleties in the data. They are essentially linear in squared returns, so genuinely nonlinear patterns can slip through. And when markets undergo structural breaks (regulatory changes, financial crises, pandemics), GARCH models can be slow to adapt.

\section{Machine Learning Approaches}

\subsection{Random Forests}

Random Forests, introduced by \textcite{breiman2001}, take a different approach. Rather than specifying a model in advance, they construct multiple decision trees from the data and average their predictions. For volatility forecasting, this typically involves training trees on lagged returns, past volatility, and other potentially relevant features.

The appeal lies in their flexibility. Random Forests can potentially capture nonlinear relationships without requiring the analyst to specify the functional form. They tend to handle outliers reasonably well and provide built-in measures of feature importance. On the downside, the forecasts tend to be smoother than realised volatility; extreme spikes may be attenuated through averaging.

\subsection{Gradient Boosting: XGBoost}

XGBoost \parencite{chen2016} builds on the ensemble idea but takes a sequential approach: each new tree focuses on correcting the mistakes of the previous ones. Regularisation helps prevent overfitting, and the algorithm handles missing data gracefully.

In volatility applications, XGBoost often does well. Some studies report R-squared values above 0.95, though such numbers should be treated with caution since they depend heavily on how the problem is set up. The model is sensitive to hyperparameter choices, and getting good performance typically requires some tuning.

\subsection{Long Short-Term Memory Networks}

LSTMs \parencite{hochreiter1997} are a type of recurrent neural network designed to capture dependencies over long sequences. Their gating mechanisms let the network decide what information to remember and what to forget, which is useful for time series where patterns may persist over many periods.

For volatility forecasting, LSTMs process sequences of past returns or realised volatility to generate predictions. They do not require explicit specification of lag structures; the network learns these from the data. \textcite{fischer2018} found that LSTMs could outperform other machine learning methods in financial applications, and \textcite{liu2019} reported similar results for equity index volatility, particularly during periods of market stress.

Whether LSTMs offer genuine improvements or have simply benefited from recent popularity remains an open question. They require more data and computation than simpler methods, and the reported improvements have not been consistent across assets or time periods.

\subsection{Hybrid Approaches}

Some researchers have tried combining the best of both worlds. \textcite{kim2018}, for instance, proposed feeding GARCH residuals into an LSTM, letting the econometric model handle the well-understood linear dynamics while the neural network picks up whatever remains.

The logic is sensible: GARCH captures the basics, LSTM learns the rest. In practice, hybrid models sometimes outperform their components, but not always. The added complexity is only worthwhile if the performance gains are real and persistent.

\section{Cryptocurrency Volatility}

\subsection{Characteristics of Crypto Markets}

Cryptocurrencies present distinct modelling challenges. Bitcoin and Ethereum exhibit volatility levels several times higher than traditional equities, with daily moves of 5\% or more occurring with some regularity \parencite{baur2018}. They trade continuously (no market close, no overnight gap), and the investor base tends to skew younger and more speculative.

Structural breaks are frequent. A tweet from a prominent figure, a regulatory crackdown in a major market, or a technical failure can send prices lurching. \textcite{dyhrberg2016} found some parallels between Bitcoin and gold as hedging instruments, but the volatility profile is distinctly higher.

\subsection{GARCH Models for Cryptocurrency}

Researchers have applied GARCH-family models to crypto with mixed success. \textcite{katsiampa2017} compared various specifications for Bitcoin and found that component GARCH models (which separate short-run and long-run volatility) fit best. This suggests that crypto volatility has both transient and persistent elements that need separate treatment.

Interestingly, the asymmetry in crypto markets sometimes runs opposite to equities. Rather than negative returns boosting volatility, some studies find that positive returns do, perhaps reflecting speculative exuberance. The leverage effect, in other words, may not apply here in the usual way.

\subsection{Machine Learning for Crypto Volatility}

Machine learning has attracted considerable attention in this space. \textcite{aras2021} compared deep learning approaches with GARCH for Bitcoin and reported that neural networks produced lower forecast errors, especially at longer horizons.

\textcite{zahid2022} experimented with hybrid GARCH-ML models and found that combining econometric structure with machine learning flexibility improved accuracy. But they also noted diminishing returns: fancier architectures did not necessarily beat simpler ones. This is a recurring theme. Complexity does not guarantee improvement.

\section{Comparative Studies and Research Gap}

\subsection{GARCH versus Machine Learning}

The literature comparing GARCH and machine learning presents a somewhat inconsistent picture. \textcite{bucci2020} found that neural networks outperformed GARCH for realised volatility on equity indices, particularly during turbulent periods. However, \textcite{christensen2023} demonstrated that simpler methods (including ridge regression) could match or exceed deep learning performance in certain settings.

\textcite{rahimikia2021} conducted a broader comparison and concluded that gradient boosting often performed well, though the best-performing model varied by asset and horizon. The emerging consensus appears to be that no universal best model exists. Context matters: GARCH may perform adequately in calm markets, while machine learning approaches might adapt better during regime transitions.

\subsection{Identified Research Gap}

For all the papers published, some gaps remain. First, most studies focus on either crypto or traditional assets, rarely both together. This makes it hard to know whether findings transfer across asset classes. Second, comparisons often pit GARCH against a single ML method (usually LSTM), leaving open where simpler alternatives like Random Forest might fit. Third, many papers emphasise in-sample fit rather than genuine out-of-sample forecasting, and when out-of-sample tests are used, the holdout periods are often short.

This thesis attempts to address these gaps by applying a consistent methodology (GARCH, EGARCH, Random Forest, XGBoost, and LSTM) to both cryptocurrencies (Bitcoin, Ethereum) and traditional assets (S\&P 500, VIX). The focus is on out-of-sample performance, evaluated with standard metrics and Diebold-Mariano tests \parencite{diebold1995} to assess whether observed differences are statistically meaningful.
